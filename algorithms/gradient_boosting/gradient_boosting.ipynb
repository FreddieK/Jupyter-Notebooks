{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBM\n",
    "The workhorse of modern machine learning. After falling down the rabbit hole of understanding the construction of regressiontree (and creating my own implementation), I am now ready to plug it into a [gradient boosting machine](https://en.wikipedia.org/wiki/Gradient_boosting).\n",
    "\n",
    "Below, my notes on the algorithm, but first some links to material that does a much better job of explaining it [here](http://explained.ai/gradient-boosting/index.html) and [here](http://www.chengli.io/tutorials/gradient_boosting.pdf).\n",
    "\n",
    "### Pseudo algorithm\n",
    "- Create an initial estimator (based on target variable mean) for data\n",
    "- Based on error in initial prediction, recursively add a regression tree that tries to minimise the error of the previous prediction\n",
    "\n",
    "### Intuition\n",
    "The additive forward expansion of the residual is really an example of gradient descent, with each expansion signifying another step taken. \n",
    "\n",
    "Using squared loss, which is the most intuitive;\n",
    "\n",
    "$$L(y, F(x)) = \\frac{(y âˆ’ F(x))^2}{2}$$\n",
    "$$J = \\sum{L(y_i, F(x_i))}$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial F(x_i)} = F(x_i) - y_i$$\n",
    "\n",
    "$$y_i - F(x_i) = - \\frac{\\partial J}{\\partial F(x_i)}$$\n",
    "\n",
    "Thus, we can reformulate;\n",
    "\n",
    "$$F(x_i) := F(x_i) + h(x_i)$$\n",
    "$$F(x_i) := F(x_i) + y - F(x_i)$$\n",
    "$$F(x_i) := F(x_i) - 1\\frac{\\partial J}{\\partial F(x_i)}$$\n",
    "$$\\theta_i := \\theta_i - \\lambda \\frac{\\partial J}{\\partial \\theta_i}$$\n",
    "\n",
    "Where $lambda$ signifies the step size.\n",
    "\n",
    "#### Using L1 (Absolute) Loss\n",
    "Alternatives to the squared loss include Absolute loss and Huber loss which both are more robust to outliers in the data.\n",
    "\n",
    "Since absolute loss derives to: \n",
    "\n",
    "$$- \\frac{\\partial J}{\\partial F(x_i)} = sign(y_i - F(x_i))$$\n",
    "\n",
    "Important to note is that the absolute loss is used when constructing the trees, but the predicted target value of the tree will be [the median of the examples](http://explained.ai/gradient-boosting/L1-loss.html#sec:1.1) in the leaf (compared to the mean for squared loss).\n",
    "\n",
    "#### GBM for Classification\n",
    "Train one model per class predicting 0, 1. I assume, you can add a softmax in the end to get something like class probabilities, where the sum of probabilities equals 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify right venv is used\n",
    "import platform\n",
    "assert platform.python_version() == '3.7.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression target variable\n",
    "So, let's get to it. To keep it simple, we'll predict a regression target variable and use L2 loss.\n",
    "\n",
    "Since my custom tree model is not very optimized (profiling the performance and optimising might be a nice next challenge, including rewriting parts in Cython), the trees are a bit slow to build so I limit to ten trees and set a relatively high learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = load_boston()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_df = pd.DataFrame(X, columns=data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/freddie.karlbom/dev/algorithms')\n",
    "from decisiontree.decisiontree import DecisionTree\n",
    "# Importing my custom regression tree class, can be found at\n",
    "# https://github.com/FreddieK/algorithms-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.mean(y) # initial model\n",
    "\n",
    "learning_rate = 0.5\n",
    "nr_trees = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.647207423956008\n",
      "4.64223543227224\n",
      "3.5060203764688502\n",
      "3.0035297382705246\n",
      "2.7370645289421143\n",
      "2.5532973608407707\n",
      "2.3876131975762247\n",
      "2.27056874997753\n",
      "2.201764300656173\n",
      "2.139500731765435\n",
      "2.0867045115081146\n"
     ]
    }
   ],
   "source": [
    "print(DecisionTree.score(y, y_pred)) # initial model mae\n",
    "\n",
    "for i in range(nr_trees):\n",
    "    dx = y - y_pred\n",
    "    tree = DecisionTree()\n",
    "    \n",
    "    dx_df = pd.DataFrame(dx)\n",
    "    dx_df.columns = ['y']\n",
    "\n",
    "    tree.build_tree(X_df, dx_df)\n",
    "    y_pred += learning_rate*np.array(tree.predict(X_df))\n",
    "    \n",
    "    mae = DecisionTree.score(y, y_pred)\n",
    "    print(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "- Seeing the mean average error decrease consistently between iterations means the model keep improving with each new tree added\n",
    "- When I decided to get more familiar with gradient boosting, I had no clue that I would end up most of the time reading up on decision trees\n",
    "- Note to self: when refactoring this code into a class for my repository, I'll store all the trees in the model so that predictions can be made on a hold-out set post training as well..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

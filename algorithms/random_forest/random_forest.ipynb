{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "Second workhorse of modern ML. Just quick notes, and then implementation.\n",
    "\n",
    "## Pseudo algorithm\n",
    "- Bootstrap M sets of data (M = number of trees)\n",
    "- For each set, fit a decision tree. At each node in the tree, features considered for splitting are randomly selected\n",
    "- Predictions are then made by averaging the output from the trees, alternatively taking the mode of the set (in classification)\n",
    "\n",
    "> In my experiments with random forests, bagging is used in tandem with random feature selection. Each new training set is drawn, with replacement, from the original training set. Then a tree is grown on the new training set using\n",
    "random feature selection.\n",
    "\n",
    "[Breiman 2001](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)\n",
    "\n",
    "## Feature Bagging\n",
    "> Random forests differ in only one way from this general scheme: they use a modified tree learning algorithm that selects, at each candidate split in the learning process, a random subset of the features.\n",
    ">\n",
    "> [...]\n",
    ">\n",
    "> Typically, for a classification problem with p features, âˆšp (rounded down) features are used in each split. For regression problems the inventors recommend p/3 (rounded down) with a minimum node size of 5 as the default.\n",
    "\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Random_forest)\n",
    "\n",
    "### Constructing Trees\n",
    "Since only subset of features are considered, trees train much faster than for example in GBM, meaning you can have a much higher number of trees without training slowing down.\n",
    "\n",
    "Extremely Random Forest (ExtraTrees) even sets the splitting point randomly.\n",
    "\n",
    "#### Using Linear Combinations\n",
    "> Another approach consists of defining more features by taking random linear combinations of a number of the input variables. That is, a feature is generated by specifying L, the number of variables to be combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/freddie.karlbom/dev/algorithms')\n",
    "from algorithms.decisiontree import DecisionTree\n",
    "# Importing my custom regression tree class, can be found at\n",
    "# https://github.com/FreddieK/algorithms-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_boston()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "df_train = pd.DataFrame(X_train, columns=data.feature_names)\n",
    "df_train['y'] = y_train\n",
    "\n",
    "df_test = pd.DataFrame(X_test, columns=data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.02340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6140</td>\n",
       "      <td>5.304</td>\n",
       "      <td>97.3</td>\n",
       "      <td>2.1007</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>349.48</td>\n",
       "      <td>24.91</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.62739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5380</td>\n",
       "      <td>5.834</td>\n",
       "      <td>56.5</td>\n",
       "      <td>4.4986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>395.62</td>\n",
       "      <td>8.47</td>\n",
       "      <td>19.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.03466</td>\n",
       "      <td>35.0</td>\n",
       "      <td>6.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4379</td>\n",
       "      <td>6.031</td>\n",
       "      <td>23.3</td>\n",
       "      <td>6.6407</td>\n",
       "      <td>1.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>16.9</td>\n",
       "      <td>362.25</td>\n",
       "      <td>7.83</td>\n",
       "      <td>19.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.05042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6140</td>\n",
       "      <td>6.103</td>\n",
       "      <td>85.1</td>\n",
       "      <td>2.0218</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>2.52</td>\n",
       "      <td>23.29</td>\n",
       "      <td>13.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.72580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5380</td>\n",
       "      <td>5.727</td>\n",
       "      <td>69.5</td>\n",
       "      <td>3.7965</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>390.95</td>\n",
       "      <td>11.28</td>\n",
       "      <td>18.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRIM    ZN  INDUS  CHAS     NOX     RM   AGE     DIS   RAD    TAX  \\\n",
       "0  15.02340   0.0  18.10   0.0  0.6140  5.304  97.3  2.1007  24.0  666.0   \n",
       "1   0.62739   0.0   8.14   0.0  0.5380  5.834  56.5  4.4986   4.0  307.0   \n",
       "2   0.03466  35.0   6.06   0.0  0.4379  6.031  23.3  6.6407   1.0  304.0   \n",
       "3   7.05042   0.0  18.10   0.0  0.6140  6.103  85.1  2.0218  24.0  666.0   \n",
       "4   0.72580   0.0   8.14   0.0  0.5380  5.727  69.5  3.7965   4.0  307.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT     y  \n",
       "0     20.2  349.48  24.91  12.0  \n",
       "1     21.0  395.62   8.47  19.9  \n",
       "2     16.9  362.25   7.83  19.4  \n",
       "3     20.2    2.52  23.29  13.4  \n",
       "4     21.0  390.95  11.28  18.2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class RandomForestTree(DecisionTree):\n",
    "    def __init__(self, nr_features = 4, max_depth=15, min_samples=2, **kwargs):\n",
    "        super(RandomForestTree, self).__init__(max_depth, **kwargs)\n",
    "        self.nr_features = nr_features\n",
    "\n",
    "    def _iterate(self, x, y, node, depth=1):\n",
    "        if depth >= self.max_depth:\n",
    "            node['value'] = y.mean()[0]\n",
    "            return\n",
    "        if len(x) <= self.min_samples:\n",
    "            node['value'] = y.mean()[0]\n",
    "            return\n",
    "        \n",
    "        best_split = None\n",
    "        \n",
    "        # Randomly choose features for tree\n",
    "        x_subset = random.sample(list(x.columns), self.nr_features)\n",
    "        for feature in x_subset:\n",
    "            potential_split = self._find_split(x[feature], x, y)\n",
    "            if (best_split is None) or \\\n",
    "                (potential_split['SSE'] < best_split['SSE']):\n",
    "                best_split = potential_split\n",
    "\n",
    "        # If a branch turned out empty, convert node into leaf\n",
    "        if len(best_split['left_y']) == 0 or len(best_split['right_y']) == 0:\n",
    "            node['value'] = y.mean()[0]\n",
    "            return\n",
    "\n",
    "        node['feature'] = best_split['feature']\n",
    "        node['split_point'] = best_split['split_point']\n",
    "        node['split_SSE'] = best_split['SSE']\n",
    "        node['depth'] = depth\n",
    "        node['left'] = {}\n",
    "        node['right'] = {}\n",
    "\n",
    "        self._iterate(best_split['left_x'], best_split['left_y'],\n",
    "                      node['left'], depth + 1)\n",
    "        self._iterate(best_split['right_x'], best_split['right_y'],\n",
    "                      node['right'], depth + 1)\n",
    "        return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    predictions = [tree.predict(x) for tree in trees]\n",
    "    predictions = np.asarray(predictions)\n",
    "    return np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : Train (MAE): 1.41, Test (MAE): 3.14\n",
      "1 : Train (MAE): 1.31, Test (MAE): 2.83\n",
      "2 : Train (MAE): 1.29, Test (MAE): 2.51\n",
      "3 : Train (MAE): 1.26, Test (MAE): 2.32\n",
      "4 : Train (MAE): 1.23, Test (MAE): 2.24\n",
      "5 : Train (MAE): 1.24, Test (MAE): 2.15\n",
      "6 : Train (MAE): 1.20, Test (MAE): 2.17\n",
      "7 : Train (MAE): 1.21, Test (MAE): 2.10\n",
      "8 : Train (MAE): 1.18, Test (MAE): 2.09\n",
      "9 : Train (MAE): 1.17, Test (MAE): 2.02\n",
      "10 : Train (MAE): 1.13, Test (MAE): 2.00\n",
      "11 : Train (MAE): 1.13, Test (MAE): 2.00\n",
      "12 : Train (MAE): 1.12, Test (MAE): 1.99\n",
      "13 : Train (MAE): 1.12, Test (MAE): 2.06\n",
      "14 : Train (MAE): 1.10, Test (MAE): 2.04\n"
     ]
    }
   ],
   "source": [
    "nr_trees = 15\n",
    "training_sets = []\n",
    "trees = []\n",
    "\n",
    "# Bootstrapping datasets for trees\n",
    "for i in range(nr_trees):\n",
    "    bootstrapped_set = df_train.sample(frac=1.0, replace=True)\n",
    "    \n",
    "    y = pd.DataFrame(bootstrapped_set['y'])\n",
    "    x = pd.DataFrame(bootstrapped_set.drop('y', axis=1))\n",
    "    \n",
    "    training_sets.append({\n",
    "        'x': x,\n",
    "        'y': y,\n",
    "    })\n",
    "    tree = RandomForestTree()\n",
    "    tree.build_tree(x, y)\n",
    "    trees.append(tree)\n",
    "    \n",
    "    # test if mae decreases as number of trees grows\n",
    "    y_pred_train = predict(df_train)\n",
    "    mae_train = np.sum(abs(y_train - y_pred_train)) / len(y_train)\n",
    "    y_pred_test = predict(df_test)\n",
    "    mae_test = np.sum(abs(y_test - y_pred_test)) / len(y_test)\n",
    "    print(f'{i} : Train (MAE): {mae_train:.2f}, Test (MAE): {mae_test:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "- **Bootstrapping should lessen the variance for the ensemble in RF compared to GBM**, since outliers in the original training set will not get included in all of the bags\n",
    "- Since the output of trees are averaged in the regression case, **the trees needs to be allowed to grow much deeper (and allow finer splits)** than in GBM in order to produce better individual predictions (and avoid having the results oscillate too wildly)\n",
    "- **With GBM you can set a relatively high learning rate (or decaying) and quickly approximate a good solution.** With Random Forest on the other hand, since each tree is just slightly better than chance, it is really about building a lot of predictors first in order to get the effect of the wisdom of the crowd.\n",
    "- **Linear combinations of features:** Mentioned briefly in the original paper, but not much elsewhere. Wonder if this is standard behaviour in common implementation?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - XGBoost\n",
    "Let's do some explorations using [XGBoost](https://github.com/dmlc/xgboost). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "filepath =  '/Users/freddiekarlbom/.kaggle/competitions/jigsaw-toxic-comment-classification-challenge/train.csv.zip'\n",
    "\n",
    "with zipfile.ZipFile(filepath) as zip:\n",
    "    with zip.open('train.csv') as myZip:\n",
    "        df = pd.read_csv(myZip) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "X = df['comment_text']\n",
    "Y = df[prediction_columns]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('vect', CountVectorizer(stop_words=\"english\")),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('clf', MultiOutputClassifier(estimator=xgb.XGBClassifier(silent=False))),\n",
    "])\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__estimator__max_depth': (3, 5),\n",
    "              'clf__estimator__learning_rate': (0.1, 0.2),\n",
    "              'clf__estimator__n_estimators': (100, 200)\n",
    "             }\n",
    "\n",
    "clf = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 21.8min\n",
      "[Parallel(n_jobs=-1)]: Done  96 out of  96 | elapsed: 52.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...g_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=False, subsample=1),\n",
       "           n_jobs=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), 'clf__estimator__max_depth': (3, 5), 'clf__estimator__learning_rate': (0.1, 0.2), 'clf__estimator__n_estimators': (100, 200)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=True)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Small subset of data to iterate more quickly...\n",
    "clf.fit(X_train[:5000], Y_train[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__learning_rate': 0.1,\n",
       " 'clf__estimator__max_depth': 3,\n",
       " 'clf__estimator__n_estimators': 100,\n",
       " 'tfidf__use_idf': True,\n",
       " 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so that 50 minute grid search just says that the default settings are the best and that the time was wasted. Time for a deep breath before we move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92800000000000005"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train[:5000], Y_train[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91214437899486156"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model = model.fit(X_train, Y_train)\n",
    "full_model.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch analysis\n",
    "Comparing to the ~90% accuracy of plain guessing, it's not a very big improvement. It's worth noticing though, that since there are so few examples of toxic comments, using a small subset of the data means there's probably not that many examples to learn from.\n",
    "\n",
    "Since there are so few toxic comments comparatively, there might be a problem with the model learning to basically always guess it's non-toxic. In order to investigate this, and see if we can create an improvement, let's create a training set with a different distribution that contains a larger share of toxic comments, and then train the most promising model using that data.\n",
    "\n",
    "This should allow the model to learn much better rules for toxic comments, but might end up giving a lot of false positives on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14604"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering out all toxic comments from the training set\n",
    "Y_train_toxic = Y_train.loc[(Y_train == 1).any(axis=1), :]\n",
    "X_train_toxic = X_train.loc[Y_train_toxic.index.values]\n",
    "Y_train_toxic.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go for ~2/3 non toxic and 1/3 toxic in the training set\n",
    "# giving us a training set roughly 1/3 of the original size\n",
    "non_toxic_samples = 30000\n",
    "Y_train_nontoxic = Y_train.loc[(Y_train == 0).all(axis=1), :]\n",
    "\n",
    "Y_train_nontoxic_sample = Y_train_nontoxic.sample(n=non_toxic_samples, random_state=1337)\n",
    "X_train_nontoxic_sample = X_train.loc[Y_train_nontoxic_sample.index.values]\n",
    "\n",
    "Y_train_sample = pd.concat([Y_train_toxic, Y_train_nontoxic_sample])\n",
    "X_train_sample = pd.concat([X_train_toxic, X_train_nontoxic_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = model.fit(X_train_sample, Y_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73033808627028962"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note - this was unexpected\n",
    "new_model.score(X_train_sample, Y_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9156622311350644"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90669256799097631"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-word level embeddings\n",
    "We saw no improvement by shifting the distribution of the training set. Instead, let's test what happens if we start looking at N-grams of chars instead of words.\n",
    "\n",
    "The benefit from this is that we will capture word stems much better. A toxic word like _fuck_ will now be recognized as the same no matter if it's _fuck_, _fucked_, _fucking_ or some other version, which might yield better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_nchar = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(3,6))),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('clf', MultiOutputClassifier(estimator=xgb.XGBClassifier())),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_nchar.fit(X_train[:5000], Y_train[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94920000000000004"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_nchar.score(X_train[:5000], Y_train[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91189372101767141"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_nchar.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nchar_df = pd.DataFrame(pipeline_nchar.predict(X_test))\n",
    "pd.crosstab(nchar_df[0], Y_test.toxic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "This sub-word level embedding approach seems promising when looking at the training set. We are clearly overfitting when omparing to the test set but given that we trained on just a small piece of data, it might generalise better if we increase the training size.\n",
    "\n",
    "After running some grid search on very small subsets, I ended up selecting the below pipeline which will be trained on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_nchar = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(3, 6))),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('clf', MultiOutputClassifier(estimator=xgb.XGBClassifier(silent=False, \n",
    "                                                                           early_stopping_rounds=5\n",
    "                                                                          ))),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.1, 0.2),\n",
    "    'vect__min_df': (10, 20)\n",
    "}\n",
    "\n",
    "clf_nchar = GridSearchCV(pipeline_nchar, parameters, n_jobs=-1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    }
   ],
   "source": [
    "clf_nchar.fit(X_train[:1000], Y_train[:1000])\n",
    "\n",
    "print(clf_nchar.score(X_train, Y_train))\n",
    "print(clf_nchar.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "- Even training on a small subset of the full data, it takes a very long time to do grid search and iterate. Learning what takes time will be key in order to avoid unneccessary waiting time and have quicker iterations. Similarly, running code in cloud rather than on my local computer.\n",
    "- I'm still going through the motions of the process quite mechanically while learning. Going forward, I need to start digging deeper into the actual featurisation\n",
    "- Pipelines are a good way to package a finished workflow, and to do grid searches, but I'm beginning to realise the importance of digging deeper into the vectorization steps, which means it would be better to initially handle them separately. This though goes all the way back to the data exploration step, that looking at things such as term frequency and what words are in there would have meant I could make more informed decisions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

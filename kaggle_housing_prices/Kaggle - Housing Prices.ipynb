{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project plan\n",
    "In order to achieve good results, the first thing should be a thorough investigation of features etc. My main goal however is to **familiarise myself with Kaggle submissions and practice using Scikit-Learn**. Thus, I'll just plow ahead with building the model with minimal time spent on data exploration and feature engineering.\n",
    "\n",
    "## Data cleaning and preparation\n",
    "* Calculate the NaN-rate for the feature\n",
    "* Drop features with high NaN-rate from the training set\n",
    "* Impute values as needed for remaining data\n",
    "* Turn factor variables into something Python can deal with\n",
    "* Split into Train/Test sets\n",
    "\n",
    "## Model building\n",
    "* Create dummy model\n",
    "* Train initial model to compare against dummy model\n",
    "* Test some other regression models quickly\n",
    "* Perform a gridsearch\n",
    "* Train model based on best parameters\n",
    "* Predict on test set and make initial Kaggle submission\n",
    "\n",
    "## Improvements\n",
    "In order to have any chance of actually building a competitive model, there would at least be some further steps I would need to look into. Since this falls outside of the actual scope I'm aiming for, I'll just note these down.\n",
    "\n",
    "* Proper data exploration, feature engineering and selection\n",
    "* Looking at Kernels submitted by other people for inspiration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_org = pd.read_csv('data/train.csv',  index_col='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = df_org['SalePrice']\n",
    "del df_org['SalePrice']\n",
    "data = df_org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nan_features(training_data):\n",
    "    '''\n",
    "    Get features with high NaN-rate from the training set\n",
    "    \n",
    "    Let's drop everything with less than 80% data coverage to ensure imputing can \n",
    "    make reasonable guesses. Call with training set, so it doesn't drop different \n",
    "    features by accident for the test set if it differs widely.\n",
    "    \n",
    "    @note: This step would make more sense after exploading out categorical features\n",
    "        so they are not affected\n",
    "    '''\n",
    "    nan_rate = training_data.notnull().sum() / training_data.shape[0]\n",
    "    \n",
    "    return nan_rate[nan_rate < 0.8].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def transform_categorical_features(data):\n",
    "    '''\n",
    "    Turn factor variables into binary features, one per category\n",
    "    \n",
    "    Looking through the data description file, there are some numerical features \n",
    "    that are really factor variables and will need to be transformed as well;\n",
    "    - MSSubClass, \n",
    "    - MoSold: Month Sold (MM)\n",
    "\n",
    "    In hindsight; dropping NAs for factor variables might've been a mistake as NA \n",
    "    can be dropped (or kept) when changing to binary features.\n",
    "    '''\n",
    "    factor_variables = ['MSSubClass', 'MSZoning', 'Street', \n",
    "                   'LotShape', 'LandContour',\n",
    "                   'Utilities', 'LotConfig', 'LandSlope', \n",
    "                   'Neighborhood', 'Condition1', 'Condition2', \n",
    "                   'BldgType', 'HouseStyle', 'RoofStyle',\n",
    "                   'RoofMatl', 'Exterior1st', 'Exterior2nd',\n",
    "                   'MasVnrType', 'ExterQual', 'ExterCond',\n",
    "                   'Foundation', 'BsmtQual', 'BsmtCond',\n",
    "                   'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n",
    "                   'Heating', 'HeatingQC', 'CentralAir',\n",
    "                   'Electrical', 'KitchenQual', 'Functional',\n",
    "                   'GarageType', 'GarageFinish', 'GarageQual',\n",
    "                   'GarageCond', 'PavedDrive', 'MoSold',\n",
    "                   'SaleType', 'SaleCondition']\n",
    "\n",
    "    return pd.get_dummies(data, columns=factor_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "def impute_missing_values(data):\n",
    "    '''\n",
    "    Impute values as needed for remaining data\n",
    "    '''    \n",
    "    impute_nan = Imputer(missing_values='NaN', strategy='median', axis=0)\n",
    "    impute_nan.fit(data)\n",
    "\n",
    "    imputed_data = impute_nan.transform(data)\n",
    "\n",
    "    imputed_df = pd.DataFrame(imputed_data)\n",
    "    imputed_df.columns = data.columns\n",
    "    imputed_df.index = data.index\n",
    "    \n",
    "    return imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oh, how I miss dplyr pipes...\n",
    "nan_features = get_nan_features(data)\n",
    "\n",
    "data = data.drop(nan_features, axis=1)\n",
    "data = transform_categorical_features(data)\n",
    "imputed_df = impute_missing_values(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(imputed_df, predictions, test_size=0.3, random_state=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lr = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86355355339558681"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_result = pd.DataFrame([int(x) for x in lr.predict(X_test)])\n",
    "lr_result.columns = ['predictions']\n",
    "lr_result['sell_price'] = y_test.reset_index().drop('Id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictions</th>\n",
       "      <th>sell_price</th>\n",
       "      <th>difference</th>\n",
       "      <th>difference_rel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>260303</td>\n",
       "      <td>259500</td>\n",
       "      <td>803</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>333908</td>\n",
       "      <td>372500</td>\n",
       "      <td>38592</td>\n",
       "      <td>10.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>111911</td>\n",
       "      <td>129500</td>\n",
       "      <td>17589</td>\n",
       "      <td>13.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68943</td>\n",
       "      <td>91000</td>\n",
       "      <td>22057</td>\n",
       "      <td>24.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>226516</td>\n",
       "      <td>171000</td>\n",
       "      <td>55516</td>\n",
       "      <td>32.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   predictions  sell_price  difference difference_rel\n",
       "0       260303      259500         803           0.31\n",
       "1       333908      372500       38592          10.36\n",
       "2       111911      129500       17589          13.58\n",
       "3        68943       91000       22057          24.24\n",
       "4       226516      171000       55516          32.47"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_result['difference'] = abs(lr_result['predictions'] - lr_result['sell_price'])\n",
    "lr_result['difference_rel'] = ['%.2f' % x for x in 100*lr_result['difference']/lr_result['sell_price']]\n",
    "lr_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decent predictions, but some are clearly outputting crazy predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78179895476400629"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Define model\n",
    "tree_model = DecisionTreeRegressor()\n",
    "\n",
    "# Fit model\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "tree_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictions</th>\n",
       "      <th>sell_price</th>\n",
       "      <th>difference</th>\n",
       "      <th>difference_rel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>190000</td>\n",
       "      <td>259500</td>\n",
       "      <td>69500</td>\n",
       "      <td>26.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>369900</td>\n",
       "      <td>372500</td>\n",
       "      <td>2600</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>110000</td>\n",
       "      <td>129500</td>\n",
       "      <td>19500</td>\n",
       "      <td>15.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>85000</td>\n",
       "      <td>91000</td>\n",
       "      <td>6000</td>\n",
       "      <td>6.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135000</td>\n",
       "      <td>171000</td>\n",
       "      <td>36000</td>\n",
       "      <td>21.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   predictions  sell_price  difference difference_rel\n",
       "0       190000      259500       69500          26.78\n",
       "1       369900      372500        2600           0.70\n",
       "2       110000      129500       19500          15.06\n",
       "3        85000       91000        6000           6.59\n",
       "4       135000      171000       36000          21.05"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_model_result = pd.DataFrame([int(x) for x in tree_model.predict(X_test)])\n",
    "tree_model_result.columns = ['predictions']\n",
    "tree_model_result['sell_price'] = y_test.reset_index().drop('Id', axis=1)\n",
    "\n",
    "tree_model_result['difference'] = abs(tree_model_result['predictions'] - tree_model_result['sell_price'])\n",
    "tree_model_result['difference_rel'] = ['%.2f' % x for x in 100*tree_model_result['difference']/tree_model_result['sell_price']]\n",
    "tree_model_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very simple model thus is about 75% accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87894955535998132"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regr_rf = RandomForestRegressor(random_state=1337)\n",
    "regr_rf.fit(X_train, y_train)\n",
    "regr_rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88727931480523126"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "regr_xgb = GradientBoostingRegressor(random_state=1337)\n",
    "regr_xgb.fit(X_train, y_train)\n",
    "regr_xgb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search optimisation\n",
    "Random forest and Gradient Boosting seem to perform about equal, and is already improving predictions. Let's do some grid search for GB to tune hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90076513816581461"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('predict', GradientBoostingRegressor(random_state=1337))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'predict__loss': ['ls', 'huber'],\n",
    "    'predict__learning_rate': [0.1, 0.05, 0.01],\n",
    "    'predict__n_estimators': [50, 100, 200],\n",
    "    'predict__max_depth': [3, 5, 7],\n",
    "    'predict__min_samples_split': [2, 3, 5]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, cv=3, param_grid=param_grid)\n",
    "grid.fit(X_train, y_train)\n",
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No simple wins here it seems when it comes to improving the results, although there's a small improvement. Let's use the settings from the best model this far and retrain a model using the full training data set, and then use that to create predictions for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('predict', GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.05, loss='ls', max_depth=5, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "             min_samples_leaf=1, min_samples_split=3,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "             presort='auto', random_state=1337, subsample=1.0, verbose=0,\n",
       "             warm_start=False))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imputed_df, predictions\n",
    "# grid.bestmodel.fit(imputed_df, predictions)\n",
    "\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.05, loss='ls', max_depth=5, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "             min_samples_leaf=1, min_samples_split=3,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "             presort='auto', random_state=1337, subsample=1.0, verbose=0,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr_xgb_tuned = GradientBoostingRegressor(learning_rate=0.05, \n",
    "                                     max_depth=5, \n",
    "                                     n_estimators=200, \n",
    "                                     min_samples_split=3, \n",
    "                                     random_state=1337)\n",
    "regr_xgb_tuned.fit(imputed_df, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions for test set\n",
    "As some categories differ between the training and test set, some columns are added/dropped to ensure the dimensions match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_org = pd.read_csv('data/test.csv',  index_col='Id')\n",
    "\n",
    "test_df = test_df_org.drop(nan_features, axis=1)\n",
    "test_df = transform_categorical_features(test_df)\n",
    "test_df = impute_missing_values(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these as 0s to test_df\n",
    "for column in set(imputed_df.columns) - set(test_df.columns):\n",
    "    test_df[column] = 0\n",
    "\n",
    "# Remove column missing from training set as it's not usable...\n",
    "for column in set(test_df.columns) - set(imputed_df.columns):\n",
    "    del test_df[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction_result = pd.DataFrame([int(x) for x in regr_xgb_tuned.predict(test_df)])\n",
    "test_prediction_result.index = test_df_org.index\n",
    "test_prediction_result.columns = ['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction_result.to_csv('xgb_tuned_predictions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
